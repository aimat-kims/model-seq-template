# PLEASE DO NOT EDIT THIS FILE
# This file is used to create a FastAPI server for model inference.

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import FileResponse, Response
from pydantic import BaseModel
from typing import List, Any, Optional, Dict
from utils.infer import run_infer, run_infer_single_model, get_model_by_id, get_next_model_id, prepare_next_model_input
from utils.config import SEQUENCE_NAME, SEQUENCE_VERSION, MODEL_SEQUENCE, INPUT_FEATURE_LIST, MODEL_PREDICTION_TEMPLATE
from time import time
import pandas as pd
import io

# Pydantic model for input/output object
class InputObject(BaseModel):
    name: str
    value: Any
    type: str

# Pydantic model for request body 
class InferenceRequest(BaseModel):
    model_input: List[InputObject]

# Pydantic model for single model inference request
class SingleModelInferenceRequest(BaseModel):
    model_id: str
    model_input: List[InputObject]

# Pydantic model for next model preparation request
class NextModelRequest(BaseModel):
    current_model_id: str
    current_output: List[InputObject]
    additional_inputs: Optional[Dict[str, Any]] = None

# Pydantic model for response body
class InferenceResponse(BaseModel):
    status: str
    results: List[InputObject]  # Same structure as input
    duration: float

# Pydantic model for sequence response
class SequenceResponse(BaseModel):
    status: str
    current_model_id: str
    current_model_name: str
    results: List[InputObject]
    duration: float
    next_model_id: Optional[str] = None
    is_sequence_complete: bool = False

# Create FastAPI app
app = FastAPI(title=f"{SEQUENCE_NAME} API", version=SEQUENCE_VERSION)

@app.get("/")
async def root():
    return FileResponse("fe/index.html")

@app.get("/style-css")
async def get_style():
    return FileResponse("fe/style.css")

@app.get("/main-js")
async def get_main_js():
    return FileResponse("fe/main.js")

@app.get("/config")
async def get_config():
    """
    Endpoint to get model sequence configuration
    """
    return {
        "sequence_name": SEQUENCE_NAME,
        "sequence_version": SEQUENCE_VERSION,
        "model_sequence": MODEL_SEQUENCE,
        # Backward compatibility
        "model_name": SEQUENCE_NAME,
        "model_version": SEQUENCE_VERSION,
        "input_features": INPUT_FEATURE_LIST,
        "prediction_template": MODEL_PREDICTION_TEMPLATE
    }

@app.get("/sequence-info")
async def get_sequence_info():
    """
    Endpoint to get detailed sequence information
    """
    sequence_info = []
    for i, model in enumerate(MODEL_SEQUENCE):
        model_info = {
            "index": i,
            "id": model["id"],
            "name": model["name"],
            "description": model["description"],
            "input_count": len(model["input_features"]),
            "output_count": len(model["output_template"]),
            "is_first": i == 0,
            "is_last": i == len(MODEL_SEQUENCE) - 1,
            "next_model_id": MODEL_SEQUENCE[i + 1]["id"] if i < len(MODEL_SEQUENCE) - 1 else None
        }
        sequence_info.append(model_info)
    
    return {
        "sequence_name": SEQUENCE_NAME,
        "sequence_version": SEQUENCE_VERSION,
        "total_models": len(MODEL_SEQUENCE),
        "models": sequence_info
    }

@app.get("/model/{model_id}")
async def get_model_config(model_id: str):
    """
    Endpoint to get configuration for a specific model
    """
    model = get_model_by_id(model_id)
    if not model:
        raise HTTPException(status_code=404, detail=f"Model with id '{model_id}' not found")
    
    return model

@app.post("/infer/", response_model=InferenceResponse)
async def infer(request: InferenceRequest):
    """
    Endpoint to run inference for the entire sequence (backward compatibility)
    Returns final results after running all models in sequence
    """
    try:
        start_time = time()
        # Convert Pydantic objects to dict format for the inference function
        model_input_dict = [item.model_dump() for item in request.model_input]
        
        # Run full sequence inference
        results = run_infer(model_input_dict)
        
        end_time = time()
        duration = end_time - start_time
        
        return InferenceResponse(
            status="success",
            results=results,
            duration=duration
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/infer-single/", response_model=SequenceResponse)
async def infer_single_model(request: SingleModelInferenceRequest):
    """
    Endpoint to run inference for a single model in the sequence
    """
    try:
        start_time = time()
        
        # Validate model exists
        model = get_model_by_id(request.model_id)
        if not model:
            raise HTTPException(status_code=404, detail=f"Model with id '{request.model_id}' not found")
        
        # Convert Pydantic objects to dict format
        model_input_dict = [item.model_dump() for item in request.model_input]
        
        # Run inference for single model
        results = run_infer_single_model(model_input_dict, request.model_id)
        
        # Get next model info
        next_model_id = get_next_model_id(request.model_id)
        is_complete = next_model_id is None
        
        end_time = time()
        duration = end_time - start_time
        
        return SequenceResponse(
            status="success",
            current_model_id=request.model_id,
            current_model_name=model["name"],
            results=results,
            duration=duration,
            next_model_id=next_model_id,
            is_sequence_complete=is_complete
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/prepare-next/")
async def prepare_next_model(request: NextModelRequest):
    """
    Endpoint to prepare input for the next model using current output
    """
    try:
        # Get next model ID
        next_model_id = get_next_model_id(request.current_model_id)
        if not next_model_id:
            raise HTTPException(status_code=400, detail="No next model in sequence")
        
        # Convert current output to dict format
        current_output_dict = [item.model_dump() for item in request.current_output]
        
        # Prepare input for next model
        next_input = prepare_next_model_input(
            current_output_dict, 
            next_model_id, 
            request.additional_inputs
        )
        
        # Get next model info
        next_model = get_model_by_id(next_model_id)
        
        return {
            "status": "success",
            "next_model_id": next_model_id,
            "next_model_name": next_model["name"],
            "next_model_description": next_model["description"],
            "prepared_input": next_input,
            "additional_inputs_needed": [
                feature for feature in next_model["input_features"]
                if not any(output["name"] == feature["name"] for output in current_output_dict)
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/infer-csv-single/")
async def infer_csv_single_model(file: UploadFile = File(...), model_id: str = Form(...)):
    """
    Endpoint to run CSV batch inference for a single model in the sequence
    Returns CSV file with input data plus current model's prediction columns
    """
    try:
        # Validate model exists
        model = get_model_by_id(model_id)
        if not model:
            raise HTTPException(status_code=404, detail=f"Model with id '{model_id}' not found")
        
        # Check if model has image inputs or outputs
        has_image_input = any(feature['type'] == 'image' for feature in model['input_features'])
        has_image_output = any(feature['type'] == 'image' for feature in model['output_template'])
        
        if has_image_input or has_image_output:
            raise HTTPException(status_code=400, detail="CSV upload is not supported for models with image inputs or outputs")
        
        # Read CSV file
        try:
            contents = await file.read()
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Failed to read CSV file: {str(e)}")
        
        # Validate CSV columns
        required_columns = [feature['name'] for feature in model['input_features']]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise HTTPException(
                status_code=400, 
                detail=f"Missing required columns: {missing_columns}. Required columns: {required_columns}"
            )
        
        # Process each row through the single model
        results_list = []
        for index, row in df.iterrows():
            try:
                # Create model input for this row
                model_input = []
                for feature in model['input_features']:
                    value = row[feature['name']]
                    
                    # Convert value based on type
                    if feature['type'] == 'int':
                        value = int(value)
                    elif feature['type'] == 'float':
                        value = float(value)
                    elif feature['type'] == 'string':
                        value = str(value)
                    
                    model_input.append({
                        'name': feature['name'],
                        'value': value,
                        'type': feature['type']
                    })
                
                # Run inference for this single model
                inference_results = run_infer_single_model(model_input, model_id)
                results_list.append(inference_results)
                
            except Exception as e:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Error processing row {index + 1}: {str(e)}"
                )
        
        # Add current model's prediction columns to the dataframe
        for feature in model['output_template']:
            column_values = []
            for result_row in results_list:
                for result_item in result_row:
                    if result_item['name'] == feature['name']:
                        # Handle different output types
                        if result_item['type'] == 'plot':
                            # For plot outputs, store a summary or skip
                            column_values.append(f"Plot_{feature['name']}")
                        else:
                            column_values.append(result_item['value'])
                        break
                else:
                    column_values.append(None)  # If prediction not found
            
            df[feature['name']] = column_values
        
        # Convert dataframe back to CSV
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        csv_content = csv_buffer.getvalue()
        
        # Generate filename for the output
        original_filename = file.filename or "input.csv"
        output_filename = f"{original_filename.replace('.csv', '')}_{model_id}_output.csv"
        
        # Return JSON response with CSV data instead of file download
        # This allows the frontend to handle the CSV and potentially pass it to next model
        return {
            "status": "success",
            "model_id": model_id,
            "model_name": model["name"],
            "csv_data": csv_content,
            "filename": output_filename,
            "rows_processed": len(df),
            "output_columns_added": [feature['name'] for feature in model['output_template']]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/infer-csv/")
async def infer_csv(file: UploadFile = File(...)):
    """
    Endpoint to run batch inference with CSV file (full sequence)
    Returns CSV file with input data plus final prediction columns
    """
    try:
        # Check if any model in sequence has image inputs or outputs
        has_image_input = any(
            feature['type'] == 'image' 
            for model in MODEL_SEQUENCE 
            for feature in model['input_features']
        )
        has_image_output = any(
            feature['type'] == 'image' 
            for model in MODEL_SEQUENCE 
            for feature in model['output_template']
        )
        
        if has_image_input or has_image_output:
            raise HTTPException(status_code=400, detail="CSV upload is not supported for models with image inputs or outputs")
        
        # Read CSV file
        try:
            contents = await file.read()
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Failed to read CSV file: {str(e)}")
        
        # Validate CSV columns (use first model's inputs)
        required_columns = [feature['name'] for feature in INPUT_FEATURE_LIST]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise HTTPException(
                status_code=400, 
                detail=f"Missing required columns: {missing_columns}. Required columns: {required_columns}"
            )
        
        # Process each row through full sequence
        results_list = []
        for index, row in df.iterrows():
            try:
                # Create model input for this row (first model)
                model_input = []
                for feature in INPUT_FEATURE_LIST:
                    value = row[feature['name']]
                    
                    # Convert value based on type
                    if feature['type'] == 'int':
                        value = int(value)
                    elif feature['type'] == 'float':
                        value = float(value)
                    elif feature['type'] == 'string':
                        value = str(value)
                    
                    model_input.append({
                        'name': feature['name'],
                        'value': value,
                        'type': feature['type']
                    })
                
                # Run full sequence inference for this row
                inference_results = run_infer(model_input)
                results_list.append(inference_results)
                
            except Exception as e:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Error processing row {index + 1}: {str(e)}"
                )
        
        # Add final prediction columns to the dataframe
        for feature in MODEL_PREDICTION_TEMPLATE:
            column_values = []
            for result_row in results_list:
                for result_item in result_row:
                    if result_item['name'] == feature['name']:
                        column_values.append(result_item['value'])
                        break
                else:
                    column_values.append(None)  # If prediction not found
            
            df[feature['name']] = column_values
        
        # Convert dataframe back to CSV
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        csv_content = csv_buffer.getvalue()
        
        # Return CSV file as response
        return Response(
            content=csv_content,
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=predictions_{file.filename}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")



