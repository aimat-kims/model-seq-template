# PLEASE DO NOT EDIT THIS FILE
# This file is used to create a FastAPI server for model inference.

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import FileResponse, Response
from pydantic import BaseModel
from typing import List, Any
from utils.infer import run_infer
from utils.config import MODEL_NAME, MODEL_VERSION, INPUT_FEATURE_LIST, MODEL_PREDICTION_TEMPLATE
from time import time
import pandas as pd
import io

# Pydantic model for input/output object
class InputObject(BaseModel):
    name: str
    value: Any
    type: str

# Pydantic model for request body 
class InferenceRequest(BaseModel):
    model_input: List[InputObject]

# Pydantic model for response body
class InferenceResponse(BaseModel):
    status: str
    results: List[InputObject]  # Same structure as input
    duration: float

# Create FastAPI app
app = FastAPI(title=f"{MODEL_NAME} API", version=MODEL_VERSION)

@app.get("/")
async def root():
    return FileResponse("fe/index.html")

@app.get("/style-css")
async def get_style():
    return FileResponse("fe/style.css")

@app.get("/main-js")
async def get_main_js():
    return FileResponse("fe/main.js")

@app.get("/config")
async def get_config():
    """
    Endpoint to get model configuration including input features and prediction template
    """
    return {
        "model_name": MODEL_NAME,
        "model_version": MODEL_VERSION,
        "input_features": INPUT_FEATURE_LIST,
        "prediction_template": MODEL_PREDICTION_TEMPLATE
    }

@app.post("/infer/", response_model=InferenceResponse)
async def infer(request: InferenceRequest):
    """
    Endpoint to run inference with custom input 
    Returns results in the same structure as input: list of objects with name, value, type
    """
    try:
        start_time = time()
        # Convert Pydantic objects to dict format for the inference function
        model_input_dict = [item.model_dump() for item in request.model_input]
        
        # Run inference with provided input
        results = run_infer(model_input_dict)
        
        end_time = time()
        duration = end_time - start_time
        
        return InferenceResponse(
            status="success",
            results=results,
            duration=duration
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/infer-csv/")
async def infer_csv(file: UploadFile = File(...)):
    """
    Endpoint to run batch inference with CSV file
    Returns CSV file with input data plus prediction columns
    """
    try:
        # Check if the model has any image inputs or outputs
        has_image_input = any(feature['type'] == 'image' for feature in INPUT_FEATURE_LIST)
        has_image_output = any(feature['type'] == 'image' for feature in MODEL_PREDICTION_TEMPLATE)
        
        if has_image_input or has_image_output:
            raise HTTPException(status_code=400, detail="CSV upload is not supported for models with image inputs or outputs")
        
        # Read CSV file
        try:
            contents = await file.read()
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Failed to read CSV file: {str(e)}")
        
        # Validate CSV columns
        required_columns = [feature['name'] for feature in INPUT_FEATURE_LIST]
        missing_columns = [col for col in required_columns if col not in df.columns]
        extra_columns = [col for col in df.columns if col not in required_columns]
        
        if missing_columns:
            raise HTTPException(
                status_code=400, 
                detail=f"Missing required columns: {missing_columns}. Required columns: {required_columns}"
            )
        
        # Process each row
        results_list = []
        for index, row in df.iterrows():
            try:
                # Create model input for this row
                model_input = []
                for feature in INPUT_FEATURE_LIST:
                    value = row[feature['name']]
                    
                    # Convert value based on type
                    if feature['type'] == 'int':
                        value = int(value)
                    elif feature['type'] == 'float':
                        value = float(value)
                    elif feature['type'] == 'string':
                        value = str(value)
                    
                    model_input.append({
                        'name': feature['name'],
                        'value': value,
                        'type': feature['type']
                    })
                
                # Run inference for this row
                inference_results = run_infer(model_input)
                results_list.append(inference_results)
                
            except Exception as e:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Error processing row {index + 1}: {str(e)}"
                )
        
        # Add prediction columns to the dataframe
        for feature in MODEL_PREDICTION_TEMPLATE:
            column_values = []
            for result_row in results_list:
                for result_item in result_row:
                    if result_item['name'] == feature['name']:
                        column_values.append(result_item['value'])
                        break
                else:
                    column_values.append(None)  # If prediction not found
            
            df[feature['name']] = column_values
        
        # Convert dataframe back to CSV
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        csv_content = csv_buffer.getvalue()
        
        # Return CSV file as response
        return Response(
            content=csv_content,
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=predictions_{file.filename}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")



